{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from PDFChatBot import PDFChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = str(uuid.uuid4()).replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'OLLAMA_API_BASE_URL' not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = config('OPENAI_API_KEY')\n",
    "OLLAMA_API_BASE_URL = os.environ['OLLAMA_API_BASE_URL'] if 'OLLAMA_API_BASE_URL' in os.environ else config('OLLAMA_API_BASE_URL')   \n",
    "LLM = os.environ['LLM'] if 'LLM' in os.environ else config('LLM')   \n",
    "EMBEDDING_MODEL = os.environ['EMBEDDING_MODEL'] if 'EMBEDDING_MODEL' in os.environ else config('EMBEDDING_MODEL')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stolli/miniforge3/envs/pdf-chat-bot/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/stolli/miniforge3/envs/pdf-chat-bot/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(f'Using embedding model: {EMBEDDING_MODEL}')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "print(f'Using LLM: {LLM}')\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_API_BASE_URL, \n",
    "    model=LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PDF Chatbot ...\n",
      "--- Loading and vectorizing PDF file ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing history aware retriever ---\n",
      "--- Initializing Q & A chain ---\n",
      "--- Initializing RAG chain ---\n"
     ]
    }
   ],
   "source": [
    "chat_bot = PDFChatBot('/Users/stolli/IT/Designing Data-Intensive Applications.pdf', embedding_model, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Streaming response ---\n",
      "{'input': 'What is partitioning?', 'chat_history': []}\n",
      "{'context': [Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='both key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning'), Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='both key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning'), Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='both key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning'), Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='both key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning')]}\n",
      "{'answer': 'Partition'}\n",
      "{'answer': 'ing'}\n",
      "{'answer': ' refers'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' process'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' dividing'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' dataset'}\n",
      "{'answer': ' or'}\n",
      "{'answer': ' data'}\n",
      "{'answer': ' storage'}\n",
      "{'answer': ' into'}\n",
      "{'answer': ' smaller'}\n",
      "{'answer': ','}\n",
      "{'answer': ' more'}\n",
      "{'answer': ' manageable'}\n",
      "{'answer': ' pieces'}\n",
      "{'answer': ' ('}\n",
      "{'answer': 'part'}\n",
      "{'answer': 'itions'}\n",
      "{'answer': ')'}\n",
      "{'answer': ' based'}\n",
      "{'answer': ' on'}\n",
      "{'answer': ' specific'}\n",
      "{'answer': ' criteria'}\n",
      "{'answer': ' such'}\n",
      "{'answer': ' as'}\n",
      "{'answer': ' range'}\n",
      "{'answer': ','}\n",
      "{'answer': ' hash'}\n",
      "{'answer': ','}\n",
      "{'answer': ' or'}\n",
      "{'answer': ' other'}\n",
      "{'answer': ' methods'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' This'}\n",
      "{'answer': ' can'}\n",
      "{'answer': ' be'}\n",
      "{'answer': ' done'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' various'}\n",
      "{'answer': ' purposes'}\n",
      "{'answer': ' like'}\n",
      "{'answer': ' improving'}\n",
      "{'answer': ' performance'}\n",
      "{'answer': ','}\n",
      "{'answer': ' reducing'}\n",
      "{'answer': ' memory'}\n",
      "{'answer': ' usage'}\n",
      "{'answer': ','}\n",
      "{'answer': ' or'}\n",
      "{'answer': ' enhancing'}\n",
      "{'answer': ' scalability'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' databases'}\n",
      "{'answer': ','}\n",
      "{'answer': ' distributed'}\n",
      "{'answer': ' systems'}\n",
      "{'answer': ','}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' data'}\n",
      "{'answer': ' processing'}\n",
      "{'answer': ' applications'}\n",
      "{'answer': '.'}\n",
      "{'answer': ''}\n"
     ]
    }
   ],
   "source": [
    "stream_response = []\n",
    "for chunk in chat_bot.stream_response('What is partitioning?', session_id):\n",
    "    stream_response.append(chunk)\n",
    "    print(chunk, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Partitioning refers to the process of dividing a dataset or data storage into smaller, more manageable pieces (partitions) based on specific criteria such as range, hash, or other methods. This can be done for various purposes like improving performance, reducing memory usage, or enhancing scalability in databases, distributed systems, and data processing applications.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([chunk['answer'] for chunk in stream_response if 'answer' in chunk.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating response ---\n"
     ]
    }
   ],
   "source": [
    "response = chat_bot.get_response('What is the book about? Please summarize it in around 20 sentences. Include a list of the most important topics', session_id=session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"Designing Data-Intensive Applications\" by Martin Kleppmann is about designing and building scalable and reliable large-scale data systems.\n",
      "\n",
      "Here's a summary:\n",
      "\n",
      "The book starts with an overview of how data has become increasingly important in modern computing, and how traditional systems are no longer sufficient to handle the scale and complexity of today's data-driven applications. The author then dives into the details of designing and implementing data-intensive systems, covering topics such as data modeling, storage, and processing.\n",
      "\n",
      "The book emphasizes the importance of thinking about data as a first-class citizen in system design, rather than an afterthought. It also highlights the need for a deep understanding of the underlying technology and the trade-offs involved in making design choices.\n",
      "\n",
      "The author discusses various approaches to storing and retrieving data, including relational databases, NoSQL databases, and distributed storage systems like Apache Cassandra and Amazon S3.\n",
      "\n",
      "He also covers topics such as data replication, consistency models (e.g. eventual consistency, linearizability), and consensus algorithms like Paxos and Raft.\n",
      "\n",
      "The book delves into the world of message queues and streaming data processing with Apache Kafka and Apache Storm.\n",
      "\n",
      "The author explores the challenges of building scalable and reliable systems, including dealing with failures, network partitions, and concurrent access to shared resources.\n",
      "\n",
      "He also discusses the importance of testing and monitoring in ensuring the reliability and performance of large-scale systems.\n",
      "\n",
      "Throughout the book, Kleppmann emphasizes the need for a deep understanding of the underlying technology and the trade-offs involved in making design choices.\n",
      "\n",
      "Here are the most important topics covered in the book:\n",
      "\n",
      "**Key Topics:**\n",
      "\n",
      "1. **Data Modeling**: Designing data structures to support efficient storage and retrieval.\n",
      "2. **Storage Systems**: Relational databases, NoSQL databases, distributed storage systems, and caching.\n",
      "3. **Consistency Models**: Eventually consistent, linearizability, and other consistency models.\n",
      "4. **Replication**: Maintaining multiple copies of data for availability and fault tolerance.\n",
      "5. **Message Queues**: Handling asynchronous communication between system components.\n",
      "6. **Streaming Data Processing**: Processing large volumes of data in real-time.\n",
      "7. **Scalability**: Designing systems to handle increasing traffic and data volume.\n",
      "8. **Reliability**: Ensuring that systems remain functional despite failures and errors.\n",
      "9. **Testing and Monitoring**: Verifying the performance and correctness of complex systems.\n",
      "\n",
      "Overall, the book provides a comprehensive guide to designing and building large-scale data-intensive applications, covering both theoretical and practical aspects of system design and implementation.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 608, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='Lamport, 345\\nlogical, 494\\nordering events, 291, 345\\nTitan (database), 50\\ntombstones, 74, 191, 456\\ntopics (messaging), 137, 440\\ntotal order, 341, 557\\nlimits of, 493\\nsequence numbers or timestamps, 344\\ntotal order broadcast, 348-352, 493, 522\\nconsensus algorithms and, 366-368\\nIndex | 587'),\n",
       " Document(metadata={'page': 612, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='About the Author\\nMartin Kleppmann  is a researcher in distributed systems at the University of Cam‐\\nbridge, UK. Previously he was a software engineer and entrepreneur at internet com‐\\npanies including LinkedIn and Rapportive, where he worked on large-scale data\\ninfrastructure. In the process he learned a few things the hard way, and he hopes this\\nbook will save you from repeating the same mistakes.\\nMartin is a regular conference speaker, blogger, and open source contributor. He\\nbelieves that profound technical ideas should be accessible to everyone, and that\\ndeeper understanding will help us develop better software.\\nColophon\\nThe animal on the cover of Designing Data-Intensive Applications  is an Indian wild\\nboar ( Sus scrofa cristatus ), a subspecies of wild boar found in India, Myanmar, Nepal,\\nSri Lanka, and Thailand. They are distinctive from European boars in that they have\\nhigher back bristles, no woolly undercoat, and a larger, straighter skull.'),\n",
       " Document(metadata={'page': 351, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\\nthey can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\\nquorum read  [16], and in ZooKeeper you need to call sync()  before the read [ 15]; see “Implementing linear‐\\nizable storage using total order broadcast” on page 350 .Relying on Linearizability\\nIn what circumstances is linearizability useful? Viewing the final score of a sporting\\nmatch is perhaps a frivolous example: a result that is outdated by a few seconds is\\nunlikely to cause any real harm in this situation. However, there a few areas in which\\nlinearizability is an important requirement for making a system work correctly.\\nLocking and leader election\\nA system that uses single-leader replication needs to ensure that there is indeed only\\none leader, not several (split brain). One way of electing a leader is to use a lock: every'),\n",
       " Document(metadata={'page': 351, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\\nthey can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\\nquorum read  [16], and in ZooKeeper you need to call sync()  before the read [ 15]; see “Implementing linear‐\\nizable storage using total order broadcast” on page 350 .Relying on Linearizability\\nIn what circumstances is linearizability useful? Viewing the final score of a sporting\\nmatch is perhaps a frivolous example: a result that is outdated by a few seconds is\\nunlikely to cause any real harm in this situation. However, there a few areas in which\\nlinearizability is an important requirement for making a system work correctly.\\nLocking and leader election\\nA system that uses single-leader replication needs to ensure that there is indeed only\\none leader, not several (split brain). One way of electing a leader is to use a lock: every')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 608, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='Lamport, 345\\nlogical, 494\\nordering events, 291, 345\\nTitan (database), 50\\ntombstones, 74, 191, 456\\ntopics (messaging), 137, 440\\ntotal order, 341, 557\\nlimits of, 493\\nsequence numbers or timestamps, 344\\ntotal order broadcast, 348-352, 493, 522\\nconsensus algorithms and, 366-368\\nIndex | 587'),\n",
       " Document(metadata={'page': 612, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='About the Author\\nMartin Kleppmann  is a researcher in distributed systems at the University of Cam‐\\nbridge, UK. Previously he was a software engineer and entrepreneur at internet com‐\\npanies including LinkedIn and Rapportive, where he worked on large-scale data\\ninfrastructure. In the process he learned a few things the hard way, and he hopes this\\nbook will save you from repeating the same mistakes.\\nMartin is a regular conference speaker, blogger, and open source contributor. He\\nbelieves that profound technical ideas should be accessible to everyone, and that\\ndeeper understanding will help us develop better software.\\nColophon\\nThe animal on the cover of Designing Data-Intensive Applications  is an Indian wild\\nboar ( Sus scrofa cristatus ), a subspecies of wild boar found in India, Myanmar, Nepal,\\nSri Lanka, and Thailand. They are distinctive from European boars in that they have\\nhigher back bristles, no woolly undercoat, and a larger, straighter skull.'),\n",
       " Document(metadata={'page': 351, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\\nthey can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\\nquorum read  [16], and in ZooKeeper you need to call sync()  before the read [ 15]; see “Implementing linear‐\\nizable storage using total order broadcast” on page 350 .Relying on Linearizability\\nIn what circumstances is linearizability useful? Viewing the final score of a sporting\\nmatch is perhaps a frivolous example: a result that is outdated by a few seconds is\\nunlikely to cause any real harm in this situation. However, there a few areas in which\\nlinearizability is an important requirement for making a system work correctly.\\nLocking and leader election\\nA system that uses single-leader replication needs to ensure that there is indeed only\\none leader, not several (split brain). One way of electing a leader is to use a lock: every')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "[res.append(x) for x in response['context'] if x not in res]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lamport, 345\\nlogical, 494\\nordering events, 291, 345\\nTitan (database), 50\\ntombstones, 74, 191, 456\\ntopics (messaging), 137, 440\\ntotal order, 341, 557\\nlimits of, 493\\nsequence numbers or timestamps, 344\\ntotal order broadcast, 348-352, 493, 522\\nconsensus algorithms and, 366-368\\nIndex | 587'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context'][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: /Users/stolli/IT/Designing Data-Intensive Applications.pdf\n",
      "Page: 608\n",
      "Content: Lamport, 345\n",
      "logical, 494\n",
      "ordering events, 291, 345\n",
      "Titan (database), 50\n",
      "tombstones, 74, 191, 456\n",
      "topics (messaging), 137, 440\n",
      "total order, 341, 557\n",
      "limits of, 493\n",
      "sequence numbers or timestamps, 344\n",
      "total order broadcast, 348-352, 493, 522\n",
      "consensus algorithms and, 366-368\n",
      "Index | 587\n",
      "\n",
      "Source: /Users/stolli/IT/Designing Data-Intensive Applications.pdf\n",
      "Page: 612\n",
      "Content: About the Author\n",
      "Martin Kleppmann  is a researcher in distributed systems at the University of Cam‐\n",
      "bridge, UK. Previously he was a software engineer and entrepreneur at internet com‐\n",
      "panies including LinkedIn and Rapportive, where he worked on large-scale data\n",
      "infrastructure. In the process he learned a few things the hard way, and he hopes this\n",
      "book will save you from repeating the same mistakes.\n",
      "Martin is a regular conference speaker, blogger, and open source contributor. He\n",
      "believes that profound technical ideas should be accessible to everyone, and that\n",
      "deeper understanding will help us develop better software.\n",
      "Colophon\n",
      "The animal on the cover of Designing Data-Intensive Applications  is an Indian wild\n",
      "boar ( Sus scrofa cristatus ), a subspecies of wild boar found in India, Myanmar, Nepal,\n",
      "Sri Lanka, and Thailand. They are distinctive from European boars in that they have\n",
      "higher back bristles, no woolly undercoat, and a larger, straighter skull.\n",
      "\n",
      "Source: /Users/stolli/IT/Designing Data-Intensive Applications.pdf\n",
      "Page: 351\n",
      "Content: iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\n",
      "they can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\n",
      "quorum read  [16], and in ZooKeeper you need to call sync()  before the read [ 15]; see “Implementing linear‐\n",
      "izable storage using total order broadcast” on page 350 .Relying on Linearizability\n",
      "In what circumstances is linearizability useful? Viewing the final score of a sporting\n",
      "match is perhaps a frivolous example: a result that is outdated by a few seconds is\n",
      "unlikely to cause any real harm in this situation. However, there a few areas in which\n",
      "linearizability is an important requirement for making a system work correctly.\n",
      "Locking and leader election\n",
      "A system that uses single-leader replication needs to ensure that there is indeed only\n",
      "one leader, not several (split brain). One way of electing a leader is to use a lock: every\n",
      "\n",
      "Source: /Users/stolli/IT/Designing Data-Intensive Applications.pdf\n",
      "Page: 351\n",
      "Content: iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\n",
      "they can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\n",
      "quorum read  [16], and in ZooKeeper you need to call sync()  before the read [ 15]; see “Implementing linear‐\n",
      "izable storage using total order broadcast” on page 350 .Relying on Linearizability\n",
      "In what circumstances is linearizability useful? Viewing the final score of a sporting\n",
      "match is perhaps a frivolous example: a result that is outdated by a few seconds is\n",
      "unlikely to cause any real harm in this situation. However, there a few areas in which\n",
      "linearizability is an important requirement for making a system work correctly.\n",
      "Locking and leader election\n",
      "A system that uses single-leader replication needs to ensure that there is indeed only\n",
      "one leader, not several (split brain). One way of electing a leader is to use a lock: every\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response['context']:\n",
    "    print(f'Source: {document.metadata[\"source\"]}')\n",
    "    print(f'Page: {document.metadata[\"page\"]}')\n",
    "    print(f'Content: {document.page_content}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_bot.get_response('What is partitioning?', session_id=session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_bot.get_response('Can you repeat the answer as structured list?', session_id=session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
