{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "import os\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: deepseek-coder-v2\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "if 'OLLAMA_API_BASE_URL' not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = config('OPENAI_API_KEY')\n",
    "OLLAMA_API_BASE_URL = os.environ['OLLAMA_API_BASE_URL'] if 'OLLAMA_API_BASE_URL' in os.environ else config('OLLAMA_API_BASE_URL')   \n",
    "LLM = os.environ['LLM'] if 'LLM' in os.environ else config('LLM')   \n",
    "EMBEDDING_MODEL = os.environ['EMBEDDING_MODEL'] if 'EMBEDDING_MODEL' in os.environ else config('EMBEDDING_MODEL')  \n",
    "\n",
    "print(f'Using LLM: {LLM}')\n",
    "print(f'Using embedding model: {EMBEDDING_MODEL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and chain model\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_API_BASE_URL, \n",
    "    model=LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(file_path, use_splitter=False):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    if use_splitter:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        return loader.load_and_split(text_splitter)\n",
    "    else:\n",
    "        return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    }
   ],
   "source": [
    "pdf_data = load_pdf_data(\"/Users/stolli/IT/Designing Data-Intensive Applications.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(pdf_data, embedding_model_name, persist_directory=\"chroma_db\"):\n",
    "    embedding = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    return Chroma.from_documents(pdf_data, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stolli/miniforge3/envs/llm-app/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/stolli/miniforge3/envs/llm-app/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorstore = create_vectorstore(pdf_data, embedding_model_name=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 238, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='The goal of partitioning is to spread the data and query load evenly across multiple\\nmachines, avoiding hot spots (nodes with disproportionately high load). This\\nrequires choosing a partitioning scheme that is appropriate to your data, and reba‐\\nlancing the partitions when nodes are added to or removed from the cluster.\\nWe discussed two main approaches to partitioning:\\n•Key range partitioning , where keys are sorted, and a partition owns all the keys\\nfrom some minimum up to some maximum. Sorting has the advantage that effi‐\\ncient range queries are possible, but there is a risk of hot spots if the application\\noften accesses keys that are close together in the sorted order.\\nIn this approach, partitions are typically rebalanced dynamically by splitting the\\nrange into two subranges when a partition gets too big.\\n•Hash partitioning , where a hash function is applied to each key, and a partition\\nowns a range of hashes. This method destroys the ordering of keys, making range\\nqueries inefficient, but may distribute load more evenly.\\nWhen partitioning by hash, it is common to create a fixed number of partitions\\nin advance, to assign several partitions to each node, and to move entire parti‐\\ntions from one node to another when nodes are added or removed. Dynamic\\npartitioning can also be used.\\nHybrid approaches are also possible, for example with a compound key: using one\\npart of the key to identify the partition and another part for the sort order.\\nWe also discussed the interaction between partitioning and secondary indexes. A sec‐\\nondary index also needs to be partitioned, and there are two methods:\\n•Document-partitioned indexes  (local indexes), where the secondary indexes are\\nstored in the same partition as the primary key and value. This means that only a\\nsingle partition needs to be updated on write, but a read of the secondary index\\nrequires a scatter/gather across all partitions.\\n•Term-partitioned indexes  (global indexes), where the secondary indexes are parti‐\\ntioned separately, using the indexed values. An entry in the secondary index may\\ninclude records from all partitions of the primary key. When a document is writ‐\\nten, several partitions of the secondary index need to be updated; however, a read\\ncan be served from a single partition.\\nFinally, we discussed techniques for routing queries to the appropriate partition,\\nwhich range from simple partition-aware load balancing to sophisticated parallel\\nquery execution engines.\\nBy design, every partition operates mostly independently—that’s what allows a parti‐\\ntioned database to scale to multiple machines. However, operations that need to write\\nSummary | 217'),\n",
       "  0.6960270404815674),\n",
       " (Document(metadata={'page': 238, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='The goal of partitioning is to spread the data and query load evenly across multiple\\nmachines, avoiding hot spots (nodes with disproportionately high load). This\\nrequires choosing a partitioning scheme that is appropriate to your data, and reba‐\\nlancing the partitions when nodes are added to or removed from the cluster.\\nWe discussed two main approaches to partitioning:\\n•Key range partitioning , where keys are sorted, and a partition owns all the keys\\nfrom some minimum up to some maximum. Sorting has the advantage that effi‐\\ncient range queries are possible, but there is a risk of hot spots if the application\\noften accesses keys that are close together in the sorted order.\\nIn this approach, partitions are typically rebalanced dynamically by splitting the\\nrange into two subranges when a partition gets too big.\\n•Hash partitioning , where a hash function is applied to each key, and a partition\\nowns a range of hashes. This method destroys the ordering of keys, making range\\nqueries inefficient, but may distribute load more evenly.\\nWhen partitioning by hash, it is common to create a fixed number of partitions\\nin advance, to assign several partitions to each node, and to move entire parti‐\\ntions from one node to another when nodes are added or removed. Dynamic\\npartitioning can also be used.\\nHybrid approaches are also possible, for example with a compound key: using one\\npart of the key to identify the partition and another part for the sort order.\\nWe also discussed the interaction between partitioning and secondary indexes. A sec‐\\nondary index also needs to be partitioned, and there are two methods:\\n•Document-partitioned indexes  (local indexes), where the secondary indexes are\\nstored in the same partition as the primary key and value. This means that only a\\nsingle partition needs to be updated on write, but a read of the secondary index\\nrequires a scatter/gather across all partitions.\\n•Term-partitioned indexes  (global indexes), where the secondary indexes are parti‐\\ntioned separately, using the indexed values. An entry in the secondary index may\\ninclude records from all partitions of the primary key. When a document is writ‐\\nten, several partitions of the secondary index need to be updated; however, a read\\ncan be served from a single partition.\\nFinally, we discussed techniques for routing queries to the appropriate partition,\\nwhich range from simple partition-aware load balancing to sophisticated parallel\\nquery execution engines.\\nBy design, every partition operates mostly independently—that’s what allows a parti‐\\ntioned database to scale to multiple machines. However, operations that need to write\\nSummary | 217'),\n",
       "  0.6960270404815674),\n",
       " (Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='Dynamic partitioning\\nFor databases that use key range partitioning (see “Partitioning by Key Range” on\\npage 202), a fixed number of partitions with fixed boundaries would be very incon‐\\nvenient: if you got the boundaries wrong, you could end up with all of the data in one\\npartition and all of the other partitions empty. Reconfiguring the partition bound‐\\naries manually would be very tedious.\\nFor that reason, key range–partitioned databases such as HBase and RethinkDB cre‐\\nate partitions dynamically. When a partition grows to exceed a configured size (on\\nHBase, the default is 10 GB), it is split into two partitions so that approximately half\\nof the data ends up on each side of the split [ 26]. Conversely, if lots of data is deleted\\nand a partition shrinks below some threshold, it can be merged with an adjacent par‐\\ntition. This process is similar to what happens at the top level of a B-tree (see “B-\\nTrees” on page 79 ).\\nEach partition is assigned to one node, and each node can handle multiple partitions,\\nlike in the case of a fixed number of partitions. After a large partition has been split,\\none of its two halves can be transferred to another node in order to balance the load.\\nIn the case of HBase, the transfer of partition files happens through HDFS, the\\nunderlying distributed filesystem [ 3].\\nAn advantage of dynamic partitioning is that the number of partitions adapts to the\\ntotal data volume. If there is only a small amount of data, a small number of parti‐\\ntions is sufficient, so overheads are small; if there is a huge amount of data, the size of\\neach individual partition is limited to a configurable maximum [ 23].\\nHowever, a caveat is that an empty database starts off with a single partition, since\\nthere is no a priori  information about where to draw the partition boundaries. While\\nthe dataset is small—until it hits the point at which the first partition is split—all\\nwrites have to be processed by a single node while the other nodes sit idle. To miti‐\\ngate this issue, HBase and MongoDB allow an initial set of partitions to be configured\\non an empty database (this is called pre-splitting ). In the case of key-range partition‐\\ning, pre-splitting requires that you already know what the key distribution is going to\\nlook like [ 4, 26].\\nDynamic partitioning is not only suitable for key range–partitioned data, but can\\nequally well be used with hash-partitioned data. MongoDB since version 2.4 supports\\nboth key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning'),\n",
       "  0.7210329174995422),\n",
       " (Document(metadata={'page': 233, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='Dynamic partitioning\\nFor databases that use key range partitioning (see “Partitioning by Key Range” on\\npage 202), a fixed number of partitions with fixed boundaries would be very incon‐\\nvenient: if you got the boundaries wrong, you could end up with all of the data in one\\npartition and all of the other partitions empty. Reconfiguring the partition bound‐\\naries manually would be very tedious.\\nFor that reason, key range–partitioned databases such as HBase and RethinkDB cre‐\\nate partitions dynamically. When a partition grows to exceed a configured size (on\\nHBase, the default is 10 GB), it is split into two partitions so that approximately half\\nof the data ends up on each side of the split [ 26]. Conversely, if lots of data is deleted\\nand a partition shrinks below some threshold, it can be merged with an adjacent par‐\\ntition. This process is similar to what happens at the top level of a B-tree (see “B-\\nTrees” on page 79 ).\\nEach partition is assigned to one node, and each node can handle multiple partitions,\\nlike in the case of a fixed number of partitions. After a large partition has been split,\\none of its two halves can be transferred to another node in order to balance the load.\\nIn the case of HBase, the transfer of partition files happens through HDFS, the\\nunderlying distributed filesystem [ 3].\\nAn advantage of dynamic partitioning is that the number of partitions adapts to the\\ntotal data volume. If there is only a small amount of data, a small number of parti‐\\ntions is sufficient, so overheads are small; if there is a huge amount of data, the size of\\neach individual partition is limited to a configurable maximum [ 23].\\nHowever, a caveat is that an empty database starts off with a single partition, since\\nthere is no a priori  information about where to draw the partition boundaries. While\\nthe dataset is small—until it hits the point at which the first partition is split—all\\nwrites have to be processed by a single node while the other nodes sit idle. To miti‐\\ngate this issue, HBase and MongoDB allow an initial set of partitions to be configured\\non an empty database (this is called pre-splitting ). In the case of key-range partition‐\\ning, pre-splitting requires that you already know what the key distribution is going to\\nlook like [ 4, 26].\\nDynamic partitioning is not only suitable for key range–partitioned data, but can\\nequally well be used with hash-partitioned data. MongoDB since version 2.4 supports\\nboth key-range and hash partitioning, and it splits partitions dynamically in either\\ncase.\\nPartitioning proportionally to nodes\\nWith dynamic partitioning, the number of partitions is proportional to the size of the\\ndataset, since the splitting and merging processes keep the size of each partition\\nbetween some fixed minimum and maximum. On the other hand, with a fixed num‐\\n212 | Chapter 6: Partitioning'),\n",
       "  0.7210329174995422),\n",
       " (Document(metadata={'page': 220, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='i. Partitioning, as discussed in this chapter, is a way of intentionally breaking a large database down into\\nsmaller ones. It has nothing to do with network partitions  (netsplits), a type of fault in the network between\\nnodes. We will discuss such faults in Chapter 8 .\\nCHAPTER 6\\nPartitioning\\nClearly, we must break away from the sequential and not limit the computers. We must\\nstate definitions and provide for priorities and descriptions of data. We must state relation‐\\nships, not procedures.\\n—Grace Murray Hopper, Management and the Computer of the Future  (1962)\\nIn Chapter 5  we discussed replication—that is, having multiple copies of the same\\ndata on different nodes. For very large datasets, or very high query throughput, that is\\nnot sufficient: we need to break the data up into partitions , also known as sharding .i\\nTerminological confusion\\nWhat we call a partition  here is called a shard  in MongoDB, Elas‐\\nticsearch, and SolrCloud; it’s known as a region  in HBase, a tablet\\nin Bigtable, a vnode  in Cassandra and Riak, and a vBucket  in\\nCouchbase. However, partitioning  is the most established term, so\\nwe’ll stick with that.\\nNormally, partitions are defined in such a way that each piece of data (each record,\\nrow, or document) belongs to exactly one partition. There are various ways of achiev‐\\ning this, which we discuss in depth in this chapter. In effect, each partition is a small\\ndatabase of its own, although the database may support operations that touch multi‐\\nple partitions at the same time.\\nThe main reason for wanting to partition data is scalability . Different partitions can\\nbe placed on different nodes in a shared-nothing cluster (see the introduction to\\n199'),\n",
       "  0.8409997224807739)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"What is partitioning?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    If you find the answer, write the answer in a concise way and add the list of pages that are used to derive the answer. \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=['context', 'question']\n",
    ")\n",
    "\n",
    "qa_interface = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever, \n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_interface.invoke(\"What is partitioning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_interface = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True, \n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is partitioning?\"\n",
    "\n",
    "result = conv_interface.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Partitions are small subsets of a larger dataset that are created to improve performance, scalability, and manageability. Each partition contains a subset of the data from the entire dataset, which can be stored on different machines or servers within a shared-nothing cluster architecture. This allows for more efficient processing and storage, as well as easier management and maintenance of the database. The main goal is to spread the data and query load evenly across multiple machines, avoiding hot spots where disproportionately high loads occur. Different partitioning schemes are used depending on factors such as the size of the dataset, types of queries being executed, and desired level of performance. Some common methods include key range partitioning and hash partitioning.\n",
      "\n",
      "Used pages: 6-19 to 6-23\n",
      "-----\n",
      "[Document(metadata={'page': 220, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='i. Partitioning, as discussed in this chapter, is a way of intentionally breaking a large database down into\\nsmaller ones. It has nothing to do with network partitions  (netsplits), a type of fault in the network between\\nnodes. We will discuss such faults in Chapter 8 .\\nCHAPTER 6\\nPartitioning\\nClearly, we must break away from the sequential and not limit the computers. We must\\nstate definitions and provide for priorities and descriptions of data. We must state relation‐\\nships, not procedures.\\n—Grace Murray Hopper, Management and the Computer of the Future  (1962)\\nIn Chapter 5  we discussed replication—that is, having multiple copies of the same\\ndata on different nodes. For very large datasets, or very high query throughput, that is\\nnot sufficient: we need to break the data up into partitions , also known as sharding .i\\nTerminological confusion\\nWhat we call a partition  here is called a shard  in MongoDB, Elas‐\\nticsearch, and SolrCloud; it’s known as a region  in HBase, a tablet'), Document(metadata={'page': 238, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='The goal of partitioning is to spread the data and query load evenly across multiple\\nmachines, avoiding hot spots (nodes with disproportionately high load). This\\nrequires choosing a partitioning scheme that is appropriate to your data, and reba‐\\nlancing the partitions when nodes are added to or removed from the cluster.\\nWe discussed two main approaches to partitioning:\\n•Key range partitioning , where keys are sorted, and a partition owns all the keys\\nfrom some minimum up to some maximum. Sorting has the advantage that effi‐\\ncient range queries are possible, but there is a risk of hot spots if the application\\noften accesses keys that are close together in the sorted order.\\nIn this approach, partitions are typically rebalanced dynamically by splitting the\\nrange into two subranges when a partition gets too big.\\n•Hash partitioning , where a hash function is applied to each key, and a partition\\nowns a range of hashes. This method destroys the ordering of keys, making range'), Document(metadata={'page': 220, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='Terminological confusion\\nWhat we call a partition  here is called a shard  in MongoDB, Elas‐\\nticsearch, and SolrCloud; it’s known as a region  in HBase, a tablet\\nin Bigtable, a vnode  in Cassandra and Riak, and a vBucket  in\\nCouchbase. However, partitioning  is the most established term, so\\nwe’ll stick with that.\\nNormally, partitions are defined in such a way that each piece of data (each record,\\nrow, or document) belongs to exactly one partition. There are various ways of achiev‐\\ning this, which we discuss in depth in this chapter. In effect, each partition is a small\\ndatabase of its own, although the database may support operations that touch multi‐\\nple partitions at the same time.\\nThe main reason for wanting to partition data is scalability . Different partitions can\\nbe placed on different nodes in a shared-nothing cluster (see the introduction to\\n199'), Document(metadata={'page': 237, 'source': '/Users/stolli/IT/Designing Data-Intensive Applications.pdf'}, page_content='overview of techniques used in parallel databases, please see the references [ 1, 33]. \\nSummary\\nIn this chapter we explored different ways of partitioning a large dataset into smaller\\nsubsets. Partitioning is necessary when you have so much data that storing and pro‐\\ncessing it on a single machine is no longer feasible.\\n216 | Chapter 6: Partitioning')]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])\n",
    "print('-----')\n",
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you repeat your answer as structured list please?\"\n",
    "\n",
    "result = conv_interface.invoke({\"question\": query, \"chat_history\": chat_history})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' El particionamiento en una base de datos puede lograrse mediante varios métodos, incluyendo el particionamiento horizontal (dividir las tablas entre varias bases de datos) y el particionamiento vertical (dividir las columnas de la tabla). Los diferentes métodos utilizados para el particionamiento pueden ser:\\n\\n1. **Particionamiento por rango**: Las filas se asignan a particiones basadas en un rango de valores, como fechas o ID de cliente.\\n2. **Particionamiento por hash**: Utiliza una función hash para distribuir las filas uniformemente entre las particiones.\\n3. **Particionamiento circular**: Similar al hashing pero con una distribución cíclica.\\n4. **Particionamiento manual**: El administrador de la base de datos asigna explícitamente cada fila a una partición específica.\\n5. **Particionamiento por lista**: Utiliza un esquema de lista para asignar filas basado en condiciones específicas, como países o regiones.\\n\\nPara más detalles y ejemplos, se puede consultar la sección correspondiente en libros especializados sobre bases de datos o artículos científicos relacionados con el tema.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
